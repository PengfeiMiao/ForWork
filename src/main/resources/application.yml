server:
  port: 8770
netty:
  host: localhost
  port: 8989

spring:
  application:
    name: forWork
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    # 配置启动自动创建数据库表
    schema: classpath:sql/log.sql
    sql-script-encoding: utf-8
    platform: mysql
    initialization-mode: always
    # 配置Druid的其他参数，以下配置必须增加一个配置文件才能有效
    # 初始化大小，最小，最大
    druid:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://116.62.202.107:3306/forwork?useUnicode=true&characterEncoding=utf-8&allowMultiQueries=true&useSSL=false
      username: root
      password: 123456
      initialSize: 5
      minIdle: 5
      maxActive: 20
      # 获取连接等待超时的时间
      maxWait: 60000
      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
      timeBetweenEvictionRunsMillis: 60000
      # 配置一个连接在池中最小生存的时间，单位是毫秒
      minEvictableIdleTimeMillis: 300000
      validationQuery: SELECT 1 FROM DUAL
      testWhileIdle: true
      testOnBorrow: false
      testOnReturn: false
      poolPreparedStatements: true
      # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
      filters: stat, wall
      # 打开PSCache，并且指定每个连接上PSCache的大小
      maxPoolPreparedStatementPerConnectionSize: 20
      # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
      connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500
      # 合并多个DruidDataSource的监控数据
      useGlobalDataSourceStat: true
      web-stat-filter:
        enabled: true
      stat-view-servlet:
        enabled: true
        login-username: admin
        login-password: admin
        reset-enable: true
        allow: 127.0.0.1
        url-pattern: /druid/*
        #============== kafka ===================
        # 指定kafka 代理地址，可以多个
  kafka:
    bootstrap-servers: 116.62.202.107:9092
    #=============== provider  =======================
    producer:
      retries: 0
      # 每次批量发送消息的数量
      batch-size: 16384
      buffer-memory: 33554432
      # 指定消息key和消息体的编解码方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    #=============== consumer  =======================
    # 指定默认消费者group id
    consumer:
      group-id: test-consumer-group
      auto-offset-reset: earliest
      enable-auto-commit: true
      auto-commit-interval: 100
      # 指定消息key和消息体的编解码方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
apache:
  rocketmq:
    namesrvAddr: 116.62.202.107:9876 #127.0.0.1:9876
    producer:
      groupName: aaa
    consumer:
      groupName: aaa
    client:
      logUseSlf4j: true
